{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://rhyme.com/assets/img/logo-dark.png\"> <h1 align=\"center\"> Support Vector Machines</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are a powerful class of supervised learning algorithms for classification and regression problems. In the context of classification, SVMs can be viewed as maximum margin linear classifiers. \n",
    "\n",
    "The SVM uses an objective which explicitly encourages low out-of-sample error (good generalization performance). The $D$ dimensional data are divided into classes by maximizing the margin between the hyperplanes for the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we assume the two classes in the data are linearly separable. Later, for non-linear boundaries, we will use the kernel trick to exploit higher (possibly infinite) dimensional $z$-spaces, where the classes are linearly separable, find the support vectors in this space and map it back to the dimensionality of our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearly separable classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "#for advanced plot styling\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "X,y = make_blobs(n_samples=50,centers=2, random_state=0, cluster_std= 0.60)\n",
    "plt.scatter(X[:,0],X[:,1], c=y, s=50, cmap=\"summer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear classifiers we know will draw a straight line between the classes. With this example, we could do this by hand. But what should strike you is that there is more than one decision boundary (lines) that can achieve minimum in-sample error.  Let's plot them below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many possible separators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1,3.5)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap=\"summer\")\n",
    "plt.plot([0.6],[2.1],\"x\",color=\"green\",markeredgewidth=2, markersize=10)\n",
    "\n",
    "#plot descision boundaries\n",
    "for m,b in [(1,0.65),(0.5,1.6),(-0.2,2.9)]:\n",
    "    plt.plot(xfit,m*xfit+b, \"-k\")\n",
    "\n",
    "#set plt x axis limits\n",
    "plt.xlim(-1,3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum margin linear classifiers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a hyperplane defined by weight $w$ and bias $b$, a linear discriminant is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^T x + b \\left\\{\\begin{matrix} \\geq 0\\ class +1& \\\\ <0\\ class -1 \\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot, we notice that for a point $x$ that is close the decision boundary at $w^T x +b =0$, a small change in $x$ can lead to a change in classification. Now assuming that the data is linearly separable, we impose that for the training data, the decision boundary should be separated from the data by some finite amount $\\epsilon ^2$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^T x + b \\left\\{\\begin{matrix} \\geq \\epsilon^2\\ class +1& \\\\ <-\\epsilon^2\\ class -1 \\end{matrix}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inequality above, we conveniently set $\\epsilon = 1$ so that a point $x_+$ from class +1 that is closest to the decision boundary satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$w^T x_+ + b =1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and a point $x_-$ from class -1 that is closest to the decision boundary satisfies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $$w^T x_- + b =-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the margins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(-1,3.5)\n",
    "plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap=\"summer\")\n",
    "\n",
    "for m,b,d in [(1,0.65,0.33),(0.5,1.6,0.55),(-0.2,2.9,0.2)]:\n",
    "    yfit = m*xfit + b\n",
    "    plt.plot(xfit,yfit,\"-k\")\n",
    "    plt.fill_between(xfit,yfit-d,yfit+d,edgecolor=\"none\",color=\"#AAAAAA\",alpha=0.4)\n",
    "    \n",
    "plt.xlim(-1,3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using SVMs, the decision boundary that maximizes this *margin* is chosen as the optimal model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) What is the (hard) margin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From vector algebra the distance from the origin along the direction $w$ to a point $x$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{w^T x}{\\sqrt{w^T w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $margin$ between the hyperplanes for the classes is the difference between the two distances along the direction of $w$ which is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{w^T x}{\\sqrt{w^T w}}(x_+ - x_-) = \\frac{2}{\\sqrt{w^T w}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the the distance between two hyperplanes, we need to minimise the length $w^T w$. We know that for each $x^n$ we have a corresponding class label $y^n \\in \\left \\{ +1, -1 \\right \\}$. So to classify the training labels correctly and maximize this margin, the optimzation problem is equivalent to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$minimize\\ \\frac{1}{2}w^Tw$ subject to the constraints  $y^n(w^Tx^n +b)\\geq 1$, and $n =1,..., N.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this formulation is a *quadratic programming* problem -- something we know how to work with. This is known as a hard margin SVM  due to the presence of the exact classification constraint \"$\\geq 1$\", which means that the points used as support vectors exactly fall on the boundary of the margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data from before, let us now train an SVM model with Scikit-Learn's suppport vector classifier. We'll defer the discussion about kernels for later in the course. For the time being, we will use a `linear` kernel and set the `C` parameter to an arbitrarily large number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC #Support Vector Classifier\n",
    "model = SVC(kernel=\"linear\",C=1E10)\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the SVM decision boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c=y,s=50,cmap=\"summer\")\n",
    "plot_svc_decision_function(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bold line dividing the data maximizes the margin between the two sets of points. Count the number of training points just touching the margin. These three points are known as the *support vectors*. These points exactly satisfying the margin are stored in the `support_vectors_` attribute of the classifier in Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "faces = fetch_lfw_people(min_faces_per_person = 60)\n",
    "print(faces.target_names)\n",
    "print(faces.images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(3,5)\n",
    "for i,axi in enumerate(ax.flat):\n",
    "    axi.imshow(faces.images[i],cmap=\"bone\")\n",
    "    axi.set(xticks=[],yticks=[],xlabel=faces.target_names[faces.target[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pca = RandomizedPCA(n_components=150, whiten=True, random_state=42)\n",
    "svc = SVC(kernel=\"rbf\",class_weight=\"balanced\")\n",
    "model = make_pipeline(pca,svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(faces.data,faces.target, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "param_grid = {\"svc__C\":[1,5,10,50],\n",
    "             \"svc__gamma\":[0.0001,0.0005,0.001,0.005]}\n",
    "\n",
    "grid = GridSearchCV(model,param_grid)\n",
    "%time grid.fit(Xtrain, ytrain)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = grid.best_estimator_\n",
    "yfit = model.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(4,6)\n",
    "for i,axi in enumerate(ax.flat):\n",
    "    axi.imshow(Xtest[i].reshape(62,47),cmap=\"bone\")\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.set_ylabel(faces.target_names[yfit[i]].split()[-1],color=\"black\" if yfit[i] == ytest[i] else \"red\")\n",
    "    \n",
    "fig.suptitle(\"Predicted Names; Incorrect Labels in Red\",size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(ytest,yfit,target_names=faces.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(ytest,yfit)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt=\"d\",cbar=False,xticklabels=faces.target_names,yticklabels=faces.target_names)\n",
    "plt.xlabel(\"True Label\")\n",
    "plt.ylabel(\"Predicted Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
